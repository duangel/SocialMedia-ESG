{"cells":[{"cell_type":"markdown","metadata":{"id":"XOzg79p_P4uG"},"source":["Pipeline for classifying the SM data with BERT models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGE2_OglPjlk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697113803982,"user_tz":-120,"elapsed":20944,"user":{"displayName":"Angela Du","userId":"17293833042315140000"}},"outputId":"853b9fa1-d882-4488-b0ba-9c4692a17c8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Master Thesis\n","candidate_sentences.csv               FindCandidates_V2_onlyESG.ipynb\n","candidate_sentences_pred_CLUSTER.csv  \u001b[0m\u001b[01;34mfull_gov\u001b[0m/\n","candidate_sentences_pred.csv          \u001b[01;34mfull_soc\u001b[0m/\n","\u001b[01;34mCB_sentence\u001b[0m/                          PreProcessing.ipynb\n","ClassificationPipeline_AD.ipynb       regressionAnalysis.ipynb\n","\u001b[01;34mdata\u001b[0m/                                 SP500.ipynb\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Master Thesis\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"RCFsU4kuhxMB"},"source":["Preprocess Instagram data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to8fjORcCM-k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696861245169,"user_tz":-120,"elapsed":33745,"user":{"displayName":"Angela Du","userId":"17293833042315140000"}},"outputId":"5c1a2732-f905-40f3-d68e-f278d7865fa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}],"source":["!pip install transformers\n","!pip install transformers datasets\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Olcv4nQOCPnR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import glob\n","from datetime import date, datetime\n","import os\n","\n","import datasets\n","from transformers import pipeline\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPqbyLFcCYmT"},"outputs":[],"source":["######## PREDICT ESG and Sentiment\n","\n","# Helper for the transformation of the output\n","def label_to_num(inp):\n","  if inp == 'risk' or inp == 'Negative' or inp == 'negative':\n","      return -1\n","\n","  if inp == 'LABEL_0' or inp == 'neutral' or inp == 'Neutral':\n","      return 0\n","\n","  elif inp == 'LABEL_1' or inp == 'opportunity' or inp == 'Positive' or inp == 'positive':\n","    return 1\n","\n","  elif inp == 'Environmental':\n","    return 'E'\n","\n","  elif inp == 'Social':\n","    return 'S'\n","\n","  elif inp == 'Governance':\n","    return 'G'\n","\n","def insert_predictions(pips, tasks, sentences, types):\n","\n","  for i, m in enumerate(pips):\n","      # Load the pipeline\n","      cb_trained = pips[i]\n","\n","      # create list of all sentences\n","      texts = [str(x) for x in sentences[\"%s\"%types].to_numpy()]\n","\n","      # classify and insert in dataframe\n","      classifications = cb_trained(texts, batch_size=64, padding=True, truncation=True)\n","\n","      sentences[tasks[i]] = [label_to_num(x[\"label\"]) for x in classifications]\n","\n","  return sentences\n","\n","def pipelines():\n","  # load tokenizer\n","  model_name = 'climatebert/distilroberta-base-climate-f'\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","  # Env: Load model\n","  PATH = 'CB_sentence'\n","  model_E = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  pip_E = pipeline('sentiment-analysis', model=model_E, tokenizer=tokenizer, max_length = 512, device=0)\n","  # Soc: Load model\n","  PATH = 'full_soc'\n","  model_S = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  pip_S = pipeline('sentiment-analysis', model=model_S, tokenizer=tokenizer, max_length = 512, device=0)\n","  # Gov: Load model\n","  PATH = 'full_gov'\n","  model_G = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  pip_G = pipeline('sentiment-analysis', model=model_G, tokenizer=tokenizer, max_length = 512, device=0)\n","\n","  # Sentiment: Load model\n","  PATH = 'yiyanghkust/finbert-esg'\n","  tokenizer = BertTokenizer.from_pretrained(PATH)\n","  model_senF = BertForSequenceClassification.from_pretrained(PATH, num_labels = 4)\n","  pip_ESGF = pipeline('text-classification', model=model_senF, tokenizer=tokenizer, max_length = 512, device=0)\n","\n","  # Sentiment: Load model\n","  PATH = 'climatebert/distilroberta-base-climate-sentiment'\n","  model_sen = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  tokenizer = AutoTokenizer.from_pretrained(model_name, max_len=512)\n","  pip_sen = pipeline('sentiment-analysis', model=model_sen, tokenizer=tokenizer, max_length = 512, device=0)\n","\n","  # Sentiment: Load model\n","  PATH = f'cardiffnlp/twitter-roberta-base-sentiment-latest'\n","  tokenizer = AutoTokenizer.from_pretrained(PATH)\n","  model_senE = AutoModelForSequenceClassification.from_pretrained(PATH)\n","  pip_senE = pipeline('sentiment-analysis', model=model_senE, tokenizer=tokenizer, max_length = 512, device=0)\n","\n","\n","  pips = [pip_E, pip_S, pip_G, pip_ESGF, pip_sen, pip_senE]\n","  tasks = [\"env\", \"soc\", \"gov\", \"ESGFin\", \"sentiment\", \"sentimentGen\"]\n","  return pips, tasks\n","\n","def predictESG(raws):\n","  pips, tasks = pipelines()\n","\n","\n","  pipsCmmts = pips[4:]\n","  tasksCmmts = tasks[4:]\n","\n","  # analyse every raw\n","  for raw in raws:\n","      # read every path\n","      name = raw[7:-9]\n","      print(\"--------------- Starting with with %s ---------------\"%name)\n","\n","      path = raw\n","\n","      sentences = pd.DataFrame()\n","\n","      try:\n","          sentences = pd.read_csv(path, index_col=0)\n","\n","      except:\n","          print(\"Used lineterminator\")\n","          try:\n","              sentences = pd.read_csv(path, lineterminator='\\n', index_col=0)\n","          except:\n","              print(f\"Did not work for {path}!\")\n","              continue\n","\n","      sentencesDescript = insert_predictions(pips, tasks, sentences, 'Description')\n","      sentencesCmmts = insert_predictions(pipsCmmts, tasksCmmts, sentences, 'PostsComments')\n","\n","      # store file\n","      sentencesDescript.to_csv(\"./data/Results/%s/predDescript.csv\"%name)\n","\n","      f = sentencesCmmts.groupby(['PostIndex', 'Description', 'PostDate']).mean()\n","      f.to_csv(\"./data/Results/%s/predCmmtsMean.csv\"%name)\n","      print(\"--------------- Done with %s ---------------\"%name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYPzplBemcnf"},"outputs":[],"source":["files = glob.glob('./data/*.csv')\n","files.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1696597887236,"user":{"displayName":"Angela Du","userId":"17293833042315140000"},"user_tz":-120},"id":"DXkD-GZlo805","outputId":"a7f5fe7d-6455-4487-c01e-273cfc1c20b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./data/DENTSPLY SIRONA INCInsta.csv',\n"," './data/INSULET CORPInsta.csv',\n"," './data/PENTAIR PLCInsta.csv',\n"," './data/PNC FINANCIAL SERVICES GROUPInsta.csv',\n"," './data/TEXAS INSTRUMENTS INCInsta.csv',\n"," './data/TRAVELERS COS INCInsta.csv',\n"," './data/TYSON FOODS INC-CL AInsta.csv',\n"," './data/UNION PACIFIC CORPInsta.csv',\n"," './data/UNITED RENTALS INCInsta.csv',\n"," './data/US BANCORPInsta.csv',\n"," './data/VISA INC-CLASS A SHARESInsta.csv',\n"," './data/WASTE MANAGEMENT INCInsta.csv',\n"," './data/WESTERN DIGITAL CORPInsta.csv',\n"," './data/WHIRLPOOL CORPInsta.csv']"]},"metadata":{},"execution_count":6}],"source":["files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBSKSAB0pN2G"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","predictESG(files)"]},{"cell_type":"markdown","source":["Set sentiment to 0 for posts with no comments"],"metadata":{"id":"Tumc4SKcSvlR"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import glob\n","from datetime import date, datetime\n","import os"],"metadata":{"id":"6rAI5QE83cbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["companies = glob.glob('./data/ProcessedComps/*.csv')\n","comps = sorted(companies)\n","comps[130:]"],"metadata":{"id":"u6BmAtKY4-wB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["companies = glob.glob('./data/ProcessedComps/*.csv')\n","comps = sorted(companies)\n","\n","for c in comps[130:]:\n","\n","  name = c[22:-9]\n","  print('--- starting for %s ---'%name)\n","\n","  df = pd.read_csv(c)\n","  dfNone = df[df['PostsComments']=='None']\n","  noneIndex = dfNone.PostIndex.tolist()\n","\n","  sent = pd.read_csv('./data/Results/%s/predCmmtsMean.csv'%name)\n","  sent.loc[sent['PostIndex'].isin(noneIndex),['sentiment','sentimentGen']] = 0\n","  sent.to_csv('./data/Results/%s/predCmmtsMean_neutralized.csv'%name)\n","  print('--- done for %s ---'%name)"],"metadata":{"id":"e8ipqqYPTJ4Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697114534120,"user_tz":-120,"elapsed":84388,"user":{"displayName":"Angela Du","userId":"17293833042315140000"}},"outputId":"348dd429-69d5-4710-dab4-7ab574f51611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- starting for PENTAIR PLC ---\n","--- done for PENTAIR PLC ---\n","--- starting for PEPSICO INC ---\n","--- done for PEPSICO INC ---\n","--- starting for PFIZER INC ---\n","--- done for PFIZER INC ---\n","--- starting for PNC FINANCIAL SERVICES GROUP ---\n","--- done for PNC FINANCIAL SERVICES GROUP ---\n","--- starting for PPG INDUSTRIES INC ---\n","--- done for PPG INDUSTRIES INC ---\n","--- starting for PROCTER & GAMBLE CO ---\n","--- done for PROCTER & GAMBLE CO ---\n","--- starting for PULTEGROUP INC ---\n","--- done for PULTEGROUP INC ---\n","--- starting for QUALCOMM INC ---\n","--- done for QUALCOMM INC ---\n","--- starting for REGENERON PHARMACEUTICALS ---\n","--- done for REGENERON PHARMACEUTICALS ---\n","--- starting for REGIONS FINANCIAL CORP ---\n","--- done for REGIONS FINANCIAL CORP ---\n","--- starting for REPUBLIC SERVICES INC ---\n","--- done for REPUBLIC SERVICES INC ---\n","--- starting for ROBERT HALF INC ---\n","--- done for ROBERT HALF INC ---\n","--- starting for ROCKWELL AUTOMATION INC ---\n","--- done for ROCKWELL AUTOMATION INC ---\n","--- starting for ROSS STORES INC ---\n","--- done for ROSS STORES INC ---\n","--- starting for RTX CORP ---\n","--- done for RTX CORP ---\n","--- starting for S&P GLOBAL INC ---\n","--- done for S&P GLOBAL INC ---\n","--- starting for SCHWAB (CHARLES) CORP ---\n","--- done for SCHWAB (CHARLES) CORP ---\n","--- starting for SEAGATE TECHNOLOGY HOLDINGS ---\n","--- done for SEAGATE TECHNOLOGY HOLDINGS ---\n","--- starting for SERVICENOW INC ---\n","--- done for SERVICENOW INC ---\n","--- starting for SHERWIN-WILLIAMS CO ---\n","--- done for SHERWIN-WILLIAMS CO ---\n","--- starting for SIMON PROPERTY GROUP INC ---\n","--- done for SIMON PROPERTY GROUP INC ---\n","--- starting for SNAP-ON INC ---\n","--- done for SNAP-ON INC ---\n","--- starting for SOLAREDGE TECHNOLOGIES INC ---\n","--- done for SOLAREDGE TECHNOLOGIES INC ---\n","--- starting for SOUTHWEST AIRLINES CO ---\n","--- done for SOUTHWEST AIRLINES CO ---\n","--- starting for STANLEY BLACK & DECKER INC ---\n","--- done for STANLEY BLACK & DECKER INC ---\n","--- starting for STARBUCKS CORP ---\n","--- done for STARBUCKS CORP ---\n","--- starting for SYNCHRONY FINANCIAL ---\n","--- done for SYNCHRONY FINANCIAL ---\n","--- starting for T ROWE PRICE GROUP INC ---\n","--- done for T ROWE PRICE GROUP INC ---\n","--- starting for T-MOBILE US INC ---\n","--- done for T-MOBILE US INC ---\n","--- starting for TARGET CORP ---\n","--- done for TARGET CORP ---\n","--- starting for TE CONNECTIVITY LTD ---\n","--- done for TE CONNECTIVITY LTD ---\n","--- starting for TESLA INC ---\n","--- done for TESLA INC ---\n","--- starting for TEXAS INSTRUMENTS INC ---\n","--- done for TEXAS INSTRUMENTS INC ---\n","--- starting for THERMO FISHER SCIENTIFIC INC ---\n","--- done for THERMO FISHER SCIENTIFIC INC ---\n","--- starting for TRACTOR SUPPLY COMPANY ---\n","--- done for TRACTOR SUPPLY COMPANY ---\n","--- starting for TRAVELERS COS INC ---\n","--- done for TRAVELERS COS INC ---\n","--- starting for TRIMBLE INC ---\n","--- done for TRIMBLE INC ---\n","--- starting for TRUIST FINANCIAL CORP ---\n","--- done for TRUIST FINANCIAL CORP ---\n","--- starting for TYSON FOODS INC-CL A ---\n","--- done for TYSON FOODS INC-CL A ---\n","--- starting for ULTA BEAUTY INC ---\n","--- done for ULTA BEAUTY INC ---\n","--- starting for UNION PACIFIC CORP ---\n","--- done for UNION PACIFIC CORP ---\n","--- starting for UNITED AIRLINES HOLDINGS INC ---\n","--- done for UNITED AIRLINES HOLDINGS INC ---\n","--- starting for UNITED PARCEL SERVICE-CL B ---\n","--- done for UNITED PARCEL SERVICE-CL B ---\n","--- starting for UNITED RENTALS INC ---\n","--- done for UNITED RENTALS INC ---\n","--- starting for US BANCORP ---\n","--- done for US BANCORP ---\n","--- starting for VERIZON COMMUNICATIONS INC ---\n","--- done for VERIZON COMMUNICATIONS INC ---\n","--- starting for VISA INC-CLASS A SHARES ---\n","--- done for VISA INC-CLASS A SHARES ---\n","--- starting for WALMART INC ---\n","--- done for WALMART INC ---\n","--- starting for WARNER BROS DISCOVERY INC ---\n","--- done for WARNER BROS DISCOVERY INC ---\n","--- starting for WASTE MANAGEMENT INC ---\n","--- done for WASTE MANAGEMENT INC ---\n","--- starting for WELLS FARGO & CO ---\n","--- done for WELLS FARGO & CO ---\n","--- starting for WESTERN DIGITAL CORP ---\n","--- done for WESTERN DIGITAL CORP ---\n","--- starting for WHIRLPOOL CORP ---\n","--- done for WHIRLPOOL CORP ---\n","--- starting for WW GRAINGER INC ---\n","--- done for WW GRAINGER INC ---\n","--- starting for WYNN RESORTS LTD ---\n","--- done for WYNN RESORTS LTD ---\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kkITsdxiCMf"},"outputs":[],"source":["def moveFile(files,names, types):\n","  for i in range(len(files)):\n","    f = files[i]\n","    n = names[i]\n","    if types == 'Description':\n","      if os.path.exists(f):\n","        os.rename(f,\"./data/Results/%s/predDescript.csv\"%n)\n","    else:\n","      if os.path.exists(f):\n","        os.rename(f,\"./data/Results/%s/predCmmtsMean.csv\"%n)\n","\n","names = []\n","for c in cmts:\n","  names.append(c[15:-27])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rhajTsNkiEfSKQuskqIQwTv3gi25TD84","timestamp":1694547705201}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}